{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Isolation Forests](#top)\n",
    "\n",
    "Isolation forest is an anomaly detection algorithm. It detects anomalies using isolation (how far a data point is to the rest of the data), rather than modelling the normal points.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Randomly select a feature and randomly select a value for that feature within its range.\n",
    "\n",
    "2. If the observation’s feature value falls above (below) the selected value, then this value becomes the new min (max) of that feature’s range.\n",
    "\n",
    "3. Check if at least one other observation has values in the range of each feature in the dataset, where some ranges were altered via step 2. If no, then the observation is isolated.\n",
    "\n",
    "4. Repeat steps 1–3 until the observation is isolated. The number of times you had to go through these steps is the isolation number. The lower the number, the more anomalous the observation is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2049280 entries, 2006-12-16 17:24:00 to 2010-11-26 21:02:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Global_active_power    float64\n",
      " 1   Global_reactive_power  float64\n",
      " 2   Voltage                float64\n",
      " 3   Global_intensity       float64\n",
      " 4   Sub_metering_1         float64\n",
      " 5   Sub_metering_2         float64\n",
      " 6   Sub_metering_3         float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 125.1 MB\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries:\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset\n",
    "df  = pd.read_csv('../linear-regression/dataset/household_power_consumption/household_power_consumption.txt',\n",
    "                    sep=';', parse_dates={'dt' : ['Date', 'Time']}, \n",
    "                    infer_datetime_format=True, low_memory=False, \n",
    "                    na_values=['nan','?'], index_col='dt')\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop the null values:\n",
    "df = df.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into the input and output columns, split into train and test datasets, then summarize the shapes of the data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2049280, 6) (2049280,)\n",
      "(1373017, 6) (676263, 6) (1373017,) (676263,)\n"
     ]
    }
   ],
   "source": [
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model Performance\n",
    "\n",
    "We will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the **mean absolute error (*MAE*)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.009\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on the raw dataset\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scikit-learn library provides a number of built-in automatic methods for identifying outliers in data.\n",
    "\n",
    "\n",
    "After a method will be defined, then it will be fit on the *training dataset*. The fit model will then predict which examples in the training dataset are *outliers* and which are not (so-called *inliers*). The outliers will then be **removed** from the training dataset, then the model will be fit on the remaining examples and evaluated on the entire test dataset.\n",
    "\n",
    "It would be invalid to fit the outlier detection method on the entire training dataset as this would result in data leakage. That is, the model would have access to data (or information about the data) in the test set not used to train the model. This may result in an optimistic estimate of model performance.\n",
    "\n",
    "We could attempt to detect outliers on “*new data*” such as the test set prior to making a prediction, but then what do we do if outliers are detected?\n",
    "\n",
    "One approach might be to return a “None” indicating that the model is unable to make a prediction on those outlier cases. This might be an interesting extension to explore that may be appropriate for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "Isolation Forest, or iForest for short, is a tree-based anomaly detection algorithm.\n",
    "\n",
    "It is based on modeling the normal data in such a way as to isolate anomalies that are both few in number and different in the feature space.\n",
    "\n",
    "… our proposed method takes advantage of two anomalies’ quantitative properties: i) they are the minority consisting of fewer instances and ii) they have attribute-values that are very different from those of normal instances.\n",
    "\n",
    "— Isolation Forest, 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library provides an implementation of Isolation Forest in the IsolationForest class.\n",
    "\n",
    "Perhaps the most important hyperparameter in the model is the “contamination” argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1.\n",
    "\n",
    "```\n",
    "identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "\n",
    "Once identified, we can remove the outliers from the training dataset.\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tying all these together, the complete example of evaluating the linear model on the household power consumption dataset with outliers identified and removed with isolation forest is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2049280 entries, 2006-12-16 17:24:00 to 2010-11-26 21:02:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Global_active_power    float64\n",
      " 1   Global_reactive_power  float64\n",
      " 2   Voltage                float64\n",
      " 3   Global_intensity       float64\n",
      " 4   Sub_metering_1         float64\n",
      " 5   Sub_metering_2         float64\n",
      " 6   Sub_metering_3         float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 125.1 MB\n",
      "(1373017, 6) (1373017,)\n",
      "(1235716, 6) (1235716,)\n",
      "MAE: 4.616\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using isolation forest\n",
    "# import necessary libraries:\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "df  = pd.read_csv('../linear-regression/dataset/household_power_consumption/household_power_consumption.txt',\n",
    "                    sep=';', parse_dates={'dt' : ['Date', 'Time']}, \n",
    "                    infer_datetime_format=True, low_memory=False, \n",
    "                    na_values=['nan','?'], index_col='dt')\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop the null values:\n",
    "df = df.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a result, we can see that that model identified and removed *137,301 outliers* and achieved a MAE of about 4.616, an decrease over the baseline that achieved a score of about 4.009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Minimum Covariance Determinant](#second)\n",
    "\n",
    "If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers.\n",
    "\n",
    "For example, if the dataset has two input variables and both are Gaussian, then the feature space forms a multi-dimensional Gaussian and knowledge of this distribution can be used to identify values far from the distribution.\n",
    "\n",
    "This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. An efficient implementation of this technique for multivariate data is known as the Minimum Covariance Determinant, or MCD for short.\n",
    "\n",
    "The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. It also serves as a convenient and efficient tool for outlier detection.\n",
    "\n",
    "— Minimum Covariance Determinant and Extensions, 2017.\n",
    "\n",
    "The scikit-learn library provides access to this method via the EllipticEnvelope class.\n",
    "\n",
    "It provides the “contamination” argument that defines the expected ratio of outliers to be observed in practice. In this case, we will set it to a value of 0.01, found with a little trial and error.\n",
    "\n",
    "```\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "```\n",
    "Once identified, the outliers can be removed from the training dataset as we did in the prior method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tying all these together, the complete example of identifying and removing outliers from the household power consumption dataset using the elliptical envelope (minimum covariant determinant) method is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2049280 entries, 2006-12-16 17:24:00 to 2010-11-26 21:02:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Global_active_power    float64\n",
      " 1   Global_reactive_power  float64\n",
      " 2   Voltage                float64\n",
      " 3   Global_intensity       float64\n",
      " 4   Sub_metering_1         float64\n",
      " 5   Sub_metering_2         float64\n",
      " 6   Sub_metering_3         float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 125.1 MB\n",
      "(1373017, 6) (1373017,)\n",
      "(1359286, 6) (1359286,)\n",
      "MAE: 4.004\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using elliptical envelope\n",
    "\n",
    "# import necessary libraries:\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "df  = pd.read_csv('../linear-regression/dataset/household_power_consumption/household_power_consumption.txt',\n",
    "                    sep=';', parse_dates={'dt' : ['Date', 'Time']}, \n",
    "                    infer_datetime_format=True, low_memory=False, \n",
    "                    na_values=['nan','?'], index_col='dt')\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop the null values:\n",
    "df = df.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, as the result we can see that the elliptical envelope method identified and removed only 13,731 outliers, resulting in a drop in MAE from 4.009 with the baseline to 4.004.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Local Outlier Factor](#third)\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space.\n",
    "\n",
    "This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality.\n",
    "\n",
    "The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers.\n",
    "\n",
    "We introduce a local outlier (LOF) for each object in the dataset, indicating its degree of outlier-ness.\n",
    "\n",
    "— LOF: Identifying Density-based Local Outliers, 2000.\n",
    "\n",
    "The scikit-learn library provides an implementation of this approach in the LocalOutlierFactor class.\n",
    "\n",
    "The model provides the “contamination” argument, that is the expected percentage of outliers in the dataset, be indicated and defaults to 0.1.\n",
    "\n",
    "```\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1373017, 6) (1373017,)\n",
      "(1340943, 6) (1340943,)\n",
      "MAE: 4.007\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using local outlier factor\n",
    "\n",
    "# import necessary libraries:\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "df  = pd.read_csv('../linear-regression/dataset/household_power_consumption/household_power_consumption.txt',\n",
    "                    sep=';', parse_dates={'dt' : ['Date', 'Time']}, \n",
    "                    infer_datetime_format=True, low_memory=False, \n",
    "                    na_values=['nan','?'], index_col='dt')\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop the null values:\n",
    "df = df.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the local outlier factor method identified and removed 32,074 outliers, resulting in a drop in MAE from 4.009 with the baseline to 4.007. Better, but not as good as isolation forest, suggesting a different set of outliers were identified and removed."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc3f958d00ea77a43ca04c78d71367e30ce701f1bb0187ecda147570cf7e24d3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
